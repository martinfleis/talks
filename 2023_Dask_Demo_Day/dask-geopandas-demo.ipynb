{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138515c-7ab1-4100-a64e-2855106fa271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183945c-a319-463f-bddb-a53b925625bb",
   "metadata": {},
   "source": [
    "# Dask-GeoPandas Demo\n",
    "\n",
    "**Martin Fleischmann**\n",
    "\n",
    "<sup>1</sup>Urban and Regional Laboratory, Charles University, CZ<br>\n",
    "<sup>2</sup>Geographic Data Science Lab, University of Liverpool, UK<br>\n",
    "\n",
    "15/06/2023, Dask Demo Day\n",
    "\n",
    "## What is GeoPandas\n",
    "\n",
    "GeoPandas, as the name suggests, extends the popular data science library [pandas](https://pandas.pydata.org) by adding support for geospatial data.\n",
    "\n",
    "The core data structure in GeoPandas is the `geopandas.GeoDataFrame`, a subclass of `pandas.DataFrame`, that can store geometry columns and perform spatial operations. The `geopandas.GeoSeries`, a subclass of `pandas.Series`, handles the geometries. Therefore, your `GeoDataFrame` is a combination of `pandas.Series`, with traditional data (numerical, boolean, text etc.), and `geopandas.GeoSeries`, with geometries (points, polygons etc.). You can have as many columns with geometries as you wish; there's no limit typical for desktop GIS software.\n",
    "\n",
    "![geodataframe schema](fig/dataframe.svg)\n",
    "\n",
    "Each `GeoSeries` can contain any geometry type (you can even mix them within a single array) and has a `GeoSeries.crs` attribute, which stores information about the projection (CRS stands for Coordinate Reference System). Therefore, each `GeoSeries` in a `GeoDataFrame` can be in a different projection, allowing you to have, for example, multiple versions (different projections) of the same geometry.\n",
    "\n",
    "Only one `GeoSeries` in a `GeoDataFrame` is considered the _active_ geometry, which means that all geometric operations applied to a `GeoDataFrame` operate on this _active_ column.\n",
    "\n",
    "But when we talk about GeoPandas, we first have to talk about pandas.\n",
    "\n",
    "## The pieces we bring together\n",
    "\n",
    "Any software that has an ambition to properly deal with geospatial data needs to cover four aspects:\n",
    "\n",
    "- *data*\n",
    "    - We just learned that this part is covered by `pandas`.\n",
    "- *geometry*\n",
    "    - Handling of the actual geometries and operations on them. In FOSS, there is one leading engine, `GEOS`, but given it is written in C++, we need a Python wrapper. That is called `shapely`.\n",
    "- *projections*\n",
    "    - We need to know which place on the Earth (or other planet) each coordinates refer to. That is what projections, or Coordinate Reference Systems (CRS), take care of. The main FOSS engine here is `PROJ`, again written in C++ and in Python wrapped by `pyproj`.\n",
    "- *reading and writing files*\n",
    "    - Geospatial world knows a lot of file formats to store the data, from Shapefile and GeoJSON to GeoParquet. The best way to read and write into them is to use `GDAL/OGR`, another C++ library, which is avaliable either via `fiona` or recently via `pyogrio`.\n",
    "\n",
    "![ecosystem diagram](fig/ecosystem.png)\n",
    "\n",
    "Let's see all of them in action within GeoPandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d032b-0777-4873-8ffc-1de92509a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968856d6-ea2b-4c59-a4f5-17eb2e720fe3",
   "metadata": {},
   "source": [
    "GeoPandas includes some built-in data, we can use them as an illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ef1dd-4492-460a-8078-1328fe4996d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = geopandas.datasets.get_path(\"naturalearth_lowres\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a09386-b7cc-4ea5-a75d-645ba941bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(path)\n",
    "world.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfb61b-ede8-4a5c-9273-8b00f2252456",
   "metadata": {},
   "source": [
    "For the sake of simplicity here, we can remove Antarctica and re-project the data to the EPSG 3857, which will not complain about measuring the area (but never use EPSG 3857 to measure the actual area as it is extremely skewed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20979c-a6bb-483f-9566-33264fdc4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = world.query(\"continent != 'Antarctica'\").to_crs(3857)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a49529-2f13-404c-a80f-0e11d07015df",
   "metadata": {},
   "source": [
    "GeoPandas GeoDataFrame can carry one or more geometry columns and brings the support of geospatial operations on these columns. Like a creation of a convex hull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4da86-9859-4a79-a2d6-465346ec9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "world['convex_hull'] = world.convex_hull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ea8c6-de4a-4019-8145-b4cff484ce9e",
   "metadata": {},
   "source": [
    "This is equal to the code above as GeoPandas exposes geometry methods of the active geometry column to the GeoDataFrame level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19f5eb-9434-4919-9e7c-72294a94e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "world['convex_hull'] = world.geometry.convex_hull  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b81b1-237a-40f4-b9ee-2a2fcf127448",
   "metadata": {},
   "source": [
    "Now you can see that we have two geometry columns stored in our `world` GeoDataFrame but only the original one is treated as an _active_ geometry (that is the one accessible directly, without getting the column first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f383e00-509f-46c2-9243-fa9c97a81fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "world.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b66f1-874b-4354-a803-801bbdad74fc",
   "metadata": {},
   "source": [
    "We can also plot the results. Both Russia and Fiji are a bit weird as they cross the anti-meridian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54ed00-44f9-4217-8ce3-9cfbf574710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = world.plot(figsize=(12, 12))\n",
    "world.convex_hull.plot(ax=ax, facecolor='none', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc478b67-8d74-4c3f-a919-2da7d3f0c43f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dask-GeoPandas\n",
    "\n",
    "Dask-GeoPandas follows exactly the same model as `dask.dataframe` adopted for scaling `pandas.DataFrame`. We have a single `dask_geopandas.GeoDataFrame`, composed of individual partitions where each is a `geopandas.GeoDataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffdbb89-49d2-48b0-883e-11f9a10ab561",
   "metadata": {},
   "source": [
    "## Create dask GeoDataFrame\n",
    "\n",
    "We have a plenty of options how to build a `dask_geopandas.GeoDataFrame`. From in-memory `geopandas.GeoDataFrame`, reading the GIS file (using pyogrio under the hood), reading GeoParquet or Feather, or from dask.dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718152d1-966b-4888-bd5d-046770a3a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec1b46-9d04-43c1-9c06-c35f09d393f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_ddf = dask_geopandas.from_geopandas(world, npartitions=4)\n",
    "world_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59406e-ac37-4d52-8b59-10c0ca3f0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_ddf_file = dask_geopandas.read_file(path, npartitions=4)\n",
    "world_ddf_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172bfce-45de-4d0b-86e9-d81654ebbb9f",
   "metadata": {},
   "source": [
    "### Partitioned IO\n",
    "\n",
    "Since we are working with individual partitions, it is useful to save the dataframe already partitioned. The ideal file format for that is a **GeoParquet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a432c3-b535-416a-8254-ffa783fb7192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world_ddf.to_parquet(\"data/world/\")\n",
    "world_ddf.to_crs(4326).to_parquet(\"data/world_4326/\")  # later we will need the dataset in EPSG:4326 so we can already prepare it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92b553-f673-41b0-8d70-7f1389271811",
   "metadata": {},
   "source": [
    "For more complex tasks, we recommend using Parquet IO as an intermediate step to avoid large task graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6da2f7-121f-4efc-b1cb-7ba3ffb37bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world_ddf = dask_geopandas.read_parquet(\"data/world/\")\n",
    "world_ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5538d8-839e-4477-91da-c8c5644218db",
   "metadata": {},
   "source": [
    "## Embarrassingly parallel computation\n",
    "\n",
    "The first type of operations where you can benefit from parallelisation is so-called embarrassingly parallel computation. That is a computation where we treat individual partitions or individual rows indenpendently of the other, meaning there is no inter-worker communication and no data need to be sent elsewhere.\n",
    "\n",
    "One example of that is a calculation of area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087947ca-f026-46ae-97ec-c8d0d08d11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = world_ddf.area\n",
    "area.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c70050-80c9-4139-a0df-edf297a31247",
   "metadata": {},
   "source": [
    "Similar one, this time returing a `dask_geopandas.GeoSeries` instead of a `dask.dataframe.Series` would be a `convex_hull` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00183a43-04a9-4337-8b9c-5470e9be74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "convex_hull = world_ddf.convex_hull\n",
    "convex_hull.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf070133-8cae-495c-9659-bc346e201eb5",
   "metadata": {},
   "source": [
    "Since both are creating a series, we can assing both as individual columns. Let's see how that changes the task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff85d3-bd9a-4437-8c6f-2bbf76556f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_ddf['area'] = world_ddf.area\n",
    "world_ddf['convex_hull'] = world_ddf.convex_hull\n",
    "world_ddf.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb088ac7-a26c-4635-984a-f47e6a5acb56",
   "metadata": {},
   "source": [
    "Finally, we can call `compute()` and get all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891011e-6184-4f5f-922a-288d4c3085d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = world_ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89ccc7-7b6a-4b20-a011-57cfbe2c86e7",
   "metadata": {},
   "source": [
    "## Spatial join\n",
    "\n",
    "If you have to deal with a large dataframes and need a spatial join, dask-geopandas can help. Let's try to illustrate the logic of spatial join on the partitioned data using the locations of airports from around the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9620c4-8bda-4827-bb2f-784a4eac62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f0c23-161c-4e8e-982b-221871a2a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = pd.read_csv(\"data/airports.csv\")\n",
    "airports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834535b-0f3e-44d1-9450-d5405633ee7c",
   "metadata": {},
   "source": [
    "The data comes as a CSV, so we first need to create a GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c3d6f-9ae0-42ce-9815-addefe9dbff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = geopandas.GeoDataFrame(\n",
    "    airports,\n",
    "    geometry=geopandas.GeoSeries.from_xy(\n",
    "        airports[\"longitude_deg\"],\n",
    "        airports[\"latitude_deg\"],\n",
    "        crs=4326,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bb6ce-2c61-4e6d-90a5-7b97a0cc1429",
   "metadata": {},
   "source": [
    "And from that, we can create a partitioned `dask_geopandas.GeoDataFrame`. Note that we could also read the CSV with dask.dataframe and create a GeoDataFrame from that using the `dask_geopandas.from_dask_dataframe` function and `dask_geopandas.points_from_xy` to create the geometry. But since it all comfortably fits in memory, we can pick whichever option we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396a9e9-8e9f-4a6f-b617-46c0d8f5cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_ddf = dask_geopandas.from_geopandas(\n",
    "    airports,\n",
    "    npartitions=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220770d-5413-4ef7-80aa-656cf7df6a4a",
   "metadata": {},
   "source": [
    "We will join the point data of airports with the `naturalearth_lowres` dataset we have stored as an already partitioned parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c689b7b-ef0d-45ce-8149-d35d5f8a27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_ddf = dask_geopandas.read_parquet(\"data/world_4326/\")\n",
    "world_ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf4c20-4be2-408b-a90b-d417b424d961",
   "metadata": {},
   "source": [
    "The API of the `sjoin` is exactly the same as you know it from geopandas. Just in this case, it currently only creates a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b7dc5-300e-4d55-aeee-1cc070f81a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = airports_ddf.sjoin(world_ddf, predicate=\"within\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645e8b3-a674-49ce-bf45-727365230cc2",
   "metadata": {},
   "source": [
    "We started from 12 partitions of `airports_ddf` and 4 partitions of `world_ddf`. Since we haven't told Dask how are these partitions spatially distributed, it just plans to do the join of each partition from one dataframe to each partition form the other one. 12x4 = 48 partitions in the end. We can easily check that with the `npartitions` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ccfc5-0b8f-4c80-a33d-26c57eda03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57981ddc-8b23-40f4-ba91-0fd8ec9bdf37",
   "metadata": {},
   "source": [
    "The whole logic can also be represented by a task graph that illusrates the inneficiency of such an approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd9a90-ab8a-4f5c-8c09-908bc9a478ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8a0e4-e225-487c-864c-e65ac586188a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spatial partitioning\n",
    "\n",
    "Luckily, dask-geopandas supports spatial partitioning. It means that it can calculate the spatial extent of each partition (as the overall convex hull of the partition) and use it internally to do smarter decisions when creating the task graph. \n",
    "\n",
    "But first, we need to calculate these paritions. This operation is done eagerly and involves immediate reading of all geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34ea68-7250-4543-bc39-12f6895b814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_ddf.calculate_spatial_partitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d18ab8-54d0-49d6-8409-59d7e0aa811c",
   "metadata": {},
   "source": [
    "The resulting `spatial_partitions` attribute is a `geopandas.GeoSeries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd50d24-1d7d-4a71-b02a-1fc881c91248",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_ddf.spatial_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3674a9-6ec7-43a9-a798-9a19472995e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "airports_ddf.spatial_partitions.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568509c4-6ec9-4ec1-b18f-f1ce87fd01ba",
   "metadata": {},
   "source": [
    "As you can see from the plot above, our partitions are not very homogenous in terms of their spatial distribution. Each contains points from nearly whole world. And that does not help with simplification of a task graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af38189-0d3a-43ae-a3e9-2a9e153dfd7c",
   "metadata": {},
   "source": [
    "### The goal\n",
    "\n",
    "We need our partitions to be spatially coherent to minimise the amount of inter-worker communication. So we have to find a way of reshuffling the data in between workers.\n",
    "\n",
    "### Hilbert curve\n",
    "\n",
    "One way of doing so is to follow the Hilbert space-filling curve, which is a 2-dimensional curve like the one below along which we can map our geometries (usually points). The distance along the Hilbert curve then represents a spatial proximity. Two points with a similar Hilbert distance are therefore ensured to be close in space.\n",
    "\n",
    "![Hilbert](fig/Hilbert-curve_rounded-gradient-animated.gif)\n",
    "\n",
    "\n",
    "(Animation by Tim Sauder, https://en.wikipedia.org/wiki/Hilbert_curve#/media/File:Hilbert-curve_rounded-gradient-animated.gif)\n",
    "\n",
    "`dask-geopandas` (as of 0.1.0) implements Hilbert curve and two other methods based on a similar concept of space-filling (Morton curve and Geohash). You can either compute them directly or let `dask-geopandas` use them under the hood in a `spatial_shuffle` method that computes the distance along the curve and uses it to reshuffle the dataframe into spatially homogenous chunks. (Note that geometries are abstracted to the midpoint of their bounding box for the purpose of measuring the distance along the curve.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28669528-2652-4e2b-aa8d-cf35dff62d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "hilbert_distance = airports_ddf.hilbert_distance()\n",
    "hilbert_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a666ba0-ddd3-4613-b345-62be2f6230e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hilbert_distance.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e567b-28ef-403f-844c-e5dc718af135",
   "metadata": {},
   "source": [
    "`spatial_shuffle` uses by default `hilbert_distance` and partitions the dataframe based on this Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c51a6b-a1e1-45fa-88d5-7f3a65e77594",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_ddf = airports_ddf.spatial_shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7836f57-6936-4377-8893-f9db018799bf",
   "metadata": {},
   "source": [
    "We can now check how the new partitions look like in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c30a1f-fb52-4da1-950e-a258098c9a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "airports_ddf.spatial_partitions.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca97d80-7752-4be3-be68-2b30b0a8e757",
   "metadata": {},
   "source": [
    "When we are reading parquet file, its metadata already contain the information on the extent of each partition and we therefore don't have to calculate them by reading all the geometries. We can quickly check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4e773-aa1b-4eba-b95e-ba4b429ac9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_ddf.spatial_partitions is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4c27e-82d5-4f11-8d27-26f5f21b2a2b",
   "metadata": {},
   "source": [
    "The world dataset has known partitions but is not spatially shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7042db0-b265-480c-8f92-87c795d5497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_ddf.spatial_partitions.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618d67d-b8dc-4aff-a6cd-20ba655d9aaf",
   "metadata": {},
   "source": [
    "Even without doing that, we can already see that the resulting number of partitions is now 33, instead of 48 as some of the joins that would result in an empty dataframe are simply filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe15d05-b8af-4a9a-8e60-4ab9f9d16917",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = airports_ddf.sjoin(world_ddf, predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b215e7-602d-4860-973d-e92e1935f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d8b9a-1702-4d5b-9870-24ecb46d1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "joined.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3027c4-21aa-4045-8d5c-a6121819deca",
   "metadata": {},
   "source": [
    "## Aggregations with dissolve\n",
    "\n",
    "Dissolve is a typical operation when you usually need to do some shuffling of data between workers to ensure that all observations within the same category (specified using the `by` keyword) end up in the same partition so they can actually be dissolved into a single polygon. As you can imagine, proper spatial partitions may help as well but in this case, they help only in the computation, not in the task graph.\n",
    "\n",
    "Let's have a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305bb94-c3e9-4257-9bed-9ec254fa6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_ddf = dask_geopandas.read_parquet(\"data/world/\")\n",
    "\n",
    "continents = world_ddf.dissolve('continent', split_out=6, sort=False)\n",
    "continents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51c375-8d58-4ccc-b69e-851bd3b67baf",
   "metadata": {},
   "source": [
    "Above, we are using the API we know from GeoPandas with one new keyword - `split_out`. That specifies into how many partitions should we send the dissolved result. The whole method is based on the `groupby`, exactly as the original one, which returns a single partition by default. We rarely want that to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9b80c-1b40-42c8-981b-af04ad492b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "continents.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a0894-90da-4506-93ed-ecd84acbf4d7",
   "metadata": {},
   "source": [
    "The task graph shows exactly what happens. Since Dask doesn't know which categories are where, it designs the task graph to move potentially shuffle data from every original partition to every new one. In reality, some of these will be empty. And the better spatial partitions we have, the more of them will be empty, hence our operation will be cheaper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e548f-4914-4d6f-aebd-6ebfeb5f78ef",
   "metadata": {},
   "source": [
    "## Limits and caveats\n",
    "\n",
    "Truth to be told, we are now playing with the version 0.1 of dask-geopandas and not everything is as polished as we would like it to be. So there are some things which are not yet fully supported.\n",
    "\n",
    "- **Overlapping computation** - With `dask.dataframe` and `dask.array` you can use `map_overlap` to do some overlapping computations for which you need observations from neighbouring partitions. With dask-geopandas, we would need this overlap to be spatial and that is not yet supported. That means that whatever depends on topology or similar operation is currently not very easy to parallelise.\n",
    "- **Spatial indexing** - While you can use spatial indexing over spatial partitions and then within individual partitions as we use it under the hood in `sjoin`, it requires a bit of low-level dask code to make it correctly run. We hope to make that easier at some point in the future. We also want to expand the use of the spatial partitioning information to more methods (currently only `sjoin` makes use of it).\n",
    "- **Memory management** - Even though Dask can work out-of-core and you may seem dask-geopandas behaving that way sometimes, we still have some unresolved memory issues due to geometries being stored as C objects, hence their actual size is not directly visible to Dask.\n",
    "\n",
    "There are also the same, or at least very similar, rules when not to use dask-geopandas as they apply to vanilla dask.dataframe (from [Dask documentation](https://docs.dask.org/en/stable/dataframe-best-practices.html)):\n",
    "\n",
    "- For data that fits into RAM, geopandas can often be faster and easier to use than Dask. While “Big Data” tools can be exciting, they are almost always worse than normal data tools while those remain appropriate. But for embarrasinlgy parallel computation, it will often bring speedup with minimal overhead.\n",
    "- Similar to above, even if you have a large dataset there may be a point in your computation where you’ve reduced things to a more manageable level. You may want to switch to (geo)pandas at this point.\n",
    "\n",
    "```\n",
    "df = dd.read_parquet('my-giant-file.parquet')\n",
    "df = df[df.name == 'Alice']              # Select a subsection\n",
    "result = df.groupby('id').value.mean()   # Reduce to a smaller size\n",
    "result = result.compute()                # Convert to pandas dataframe\n",
    "result...                                # Continue working with pandas\n",
    "```\n",
    "\n",
    "- Usual pandas and GeoPandas performance tips like avoiding apply, using vectorized operations, using categoricals, etc., all apply equally to Dask DataFrame and dask-geopandas.\n",
    "\n",
    "See more best practices in the [Dask documentation](https://docs.dask.org/en/stable/dataframe-best-practices.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
